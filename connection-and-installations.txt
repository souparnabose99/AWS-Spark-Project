Connection & Installation details:

AWS Cloud:

Download new keypair from AWS instance creation as newSpark3.pem
Convert it to .ppk file using puttygen, select RSA, and save as puttyspark3

Host: ubuntu@ec2-3-18-111-123.us-east-2.compute.amazonaws.com
port: 22
Auth: Private Key->puttyspark3, load the private key and then connect

Installations and Commands:

Python installation on AWS:

python --version
python3 --version
sudo apt update

Anaconda installation:

wget http://repo.continuum.io/archive/Anaconda3-4.1.1-Linux-x86_64.sh

bash Anaconda3-4.1.1-Linux-x86_64.sh , bash->for executing the command

which python

jupyter notebook --generate-config

$ source .bashrc

Create certifications for our connections:

mkdir certs
cd certs

sudo openssl req -x509 -nodes -days 365 -newkey rsa:1024 -keyout mycert.pem -out mycert.pem

cd ~/.jupyter/
vi jupyter_notebook_config.py

c = get_config()

# Notebook config this is where you saved your pem cert
c.NotebookApp.certfile = u'/home/ubuntu/certs/mycert.pem' 
# Run on all IP addresses of your instance
c.NotebookApp.ip = '*'
# Don't open browser by default
c.NotebookApp.open_browser = False  
# Fix port to 8888
c.NotebookApp.port = 8888

:wq

:wq!

q!


Ubuntu 16.0.4:

ubuntu@ip-172-31-33-50:~$ pwd
/home/ubuntu
ubuntu@ip-172-31-33-50:~$ ls -la
total 598120
drwxr-xr-x 12 ubuntu ubuntu      4096 Dec 11 07:22 .
drwxr-xr-x  3 root   root        4096 Dec 10 05:19 ..
drwxrwxr-x 12 ubuntu ubuntu      4096 Dec 11 04:31 anaconda3
-rw-rw-r--  1 ubuntu ubuntu 425991075 Jan 30  2017 Anaconda3-4.1.1-Linux-x86_64.sh
-rw-------  1 ubuntu ubuntu       515 Dec 10 07:36 .bash_history
-rw-r--r--  1 ubuntu ubuntu       220 Aug 31  2015 .bash_logout
-rw-r--r--  1 ubuntu ubuntu      3856 Dec 10 05:37 .bashrc
-rw-r--r--  1 ubuntu ubuntu      3771 Dec 10 05:37 .bashrc-anaconda3.bak
drwx------  3 ubuntu ubuntu      4096 Dec 11 04:32 .cache
drwxrwxr-x  2 ubuntu ubuntu      4096 Dec 10 05:40 certs
drwxrwxr-x  2 ubuntu ubuntu      4096 Dec 10 05:36 .continuum
-rw-rw-r--  1 ubuntu ubuntu        57 Dec 11 06:04 example2.txt
-rw-rw-r--  1 ubuntu ubuntu        56 Dec 11 05:19 example.txt
drwxr-xr-x  2 ubuntu ubuntu      4096 Dec 11 06:01 .ipynb_checkpoints
drwxr-xr-x  5 ubuntu ubuntu      4096 Dec 11 04:39 .ipython
drwx------  2 ubuntu ubuntu      4096 Dec 10 05:42 .jupyter
-rw-rw-r--  1 ubuntu ubuntu      3412 Dec 11 05:14 Lambda-expression-basics.ipynb
drwx------  3 ubuntu ubuntu      4096 Dec 11 04:39 .local
-rw-r--r--  1 ubuntu ubuntu       655 Jul 12  2019 .profile
-rw-------  1 ubuntu ubuntu         7 Dec 10 05:39 .python_history
-rw-rw-r--  1 ubuntu ubuntu     19161 Dec 11 07:22 RDD-Transformation-and-Actions.ipynb
-rw-------  1 root   root        1024 Dec 10 05:40 .rnd
-rw-rw-r--  1 ubuntu ubuntu       454 Dec 11 06:43 services.txt
drwxr-xr-x 12    500    500      4096 Jul 19  2016 spark-2.0.0-bin-hadoop2.7
-rw-rw-r--  1 ubuntu ubuntu 186354175 Jul 26  2016 spark-2.0.0-bin-hadoop2.7.tgz
-rw-rw-r--  1 ubuntu ubuntu      4657 Dec 11 05:32 Spark-with-Python.ipynb
drwx------  2 ubuntu ubuntu      4096 Dec 10 05:19 .ssh
-rw-r--r--  1 ubuntu ubuntu         0 Dec 10 05:40 .sudo_as_admin_successful
-rw-------  1 ubuntu ubuntu       736 Dec 10 05:42 .viminfo


sudo apt-get update
sudo apt-get install default-jre
java -version
sudo apt-get install scala
scala -version

We need to install the python library py4j, in order to this we need to make sure that pip install is connected to our Anaconda installation of Python instead of Ubuntuâ€™s default

conda install pip
which pip

wget http://archive.apache.org/dist/spark/spark-2.0.0/spark-2.0.0-bin-hadoop2.7.tgz

$ export SPARK_HOME='/home/ubuntu/spark-2.0.0-bin-hadoop2.7'
$ export PATH=$SPARK_HOME:$PATH
$ export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH
